{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc956379-1b2f-4096-84ab-699dc27aa45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41e5cc-77c6-4bac-93fb-0eed83e0cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade torch transformers av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6d63b-6605-44b7-846d-1ed609d931a9",
   "metadata": {},
   "source": [
    "# audio from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dce36a-3302-4b4d-94e9-f462d63a87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_audio(video_file_path, audio_file_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file and saves it as an MP3 file.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_file_path: The path to the video file.\n",
    "    - audio_file_path: The path where the extracted audio file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    video = VideoFileClip(video_file_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_file_path)\n",
    "    video.close()\n",
    "\n",
    "video_file_path = '/home/girish/Downloads/Zenodo/VideoInterview/CFWyWfu_SpR.mp4'\n",
    "audio_file_path = '/home/girish/Downloads/Zenodo/Audios/output_audio.mp3'\n",
    "extract_audio(video_file_path, audio_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a815219-b7f0-48c9-949d-5f11da00ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_audio_ffmpeg(video_file_path, audio_file_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file using FFmpeg and saves it as an MP3 file.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_file_path: The path to the video file.\n",
    "    - audio_file_path: The path where the extracted audio file will be saved.\n",
    "    \"\"\"\n",
    "    command = ['ffmpeg', '-i', video_file_path, '-q:a', '0', '-map', 'a', audio_file_path]\n",
    "    \n",
    "\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "video_file_path = '/home/girish/Downloads/Zenodo/VideoInterview/CFWyWfu_SpR.mp4'\n",
    "audio_file_path = '/home/girish/Downloads/Zenodo/Audios/output_audio2.mp3'\n",
    "extract_audio_ffmpeg(video_file_path, audio_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973936d5-decf-4743-9995-b1781585275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from speechbrain.pretrained.interfaces import foreign_class\n",
    "\n",
    "\n",
    "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\n",
    "\n",
    "def extract_features(path):\n",
    "    signal, fs = torchaudio.load(path)\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    return np.array(embeddings.mean(axis=0).squeeze())\n",
    "\n",
    "def process_and_save_audio_features(folder_path):\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".wav\"):  \n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            \n",
    "            df = pd.DataFrame(features).transpose()  \n",
    "\n",
    "            csv_file_name = os.path.splitext(file)[0] + '_features.csv'\n",
    " \n",
    "            df.to_csv(os.path.join(folder_path, csv_file_name), index=False)\n",
    "\n",
    "folder_path = 'path/to/your/audio/folder'\n",
    "process_and_save_audio_features(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e2aa7-480a-49ff-a30c-811d82be4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "input_folder = '/home/girish/Downloads/Zenodo/Audios'\n",
    "output_folder = '/home/girish/Downloads/Zenodo/Audio_features'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "processed_files = [os.path.splitext(f)[0] for f in os.listdir(output_folder) if f.endswith('.csv')]\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_file = os.path.join(input_folder, filename)\n",
    "        \n",
    "        if os.path.splitext(filename)[0] in processed_files:\n",
    "            print(f\"Features for {filename} already extracted, skipping...\")\n",
    "            continue\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "            sample_rate = 16000 \n",
    "\n",
    "        inputs = feature_extractor(waveform.squeeze(), return_tensors=\"pt\", padding=\"longest\", sampling_rate=sample_rate)\n",
    "        with torch.no_grad():\n",
    "            features = model(**inputs).last_hidden_state\n",
    "\n",
    "        features_np = features.squeeze().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(features_np)\n",
    "\n",
    "        csv_filename = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_features.csv\")\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"Features saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3558a7e-41e7-4d49-88bb-f0bdc3e384fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Video Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f1cd0-f2c6-4f2f-acbc-9c68858fc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3ea83-b71d-4656-ae20-5dd01782f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 1.05MB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████| 725/725 [00:00<00:00, 2.52MB/s]\n",
      "pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████| 377M/377M [00:42<00:00, 8.94MB/s]\n",
      "/home/girish/.local/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /home/girish/Downloads/Zenodo/Video_features/video_features.csv\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "file_path = \"/home/girish/Downloads/Zenodo/VideoInterview/WfTccuO_NXE.mp4\"\n",
    "container = av.open(file_path)\n",
    "\n",
    "# Sample frames from the video\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "features_numpy = last_hidden_states.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "features_df = pd.DataFrame(features_numpy)\n",
    "features_csv_path = '/home/girish/Downloads/Zenodo/Video_features/video_features.csv'\n",
    "features_df.to_csv(features_csv_path, index=False)\n",
    "\n",
    "print(f\"Features saved to {features_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcec0e-b32e-4ecc-b747-4f4c317e0afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /home/girish/Downloads/Zenodo/Video_features/zzXHxHz_WTr.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import av\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def extract_video_features(file_path, device):\n",
    "    container = av.open(file_path)\n",
    "    indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "    \n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "video_files = [\"/home/girish/Downloads/Zenodo/VideoInterview/zzXHxHz_WTr.mp4\"]  \n",
    "all_features = []\n",
    "for file_path in video_files:\n",
    "    features = extract_video_features(file_path, device)\n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "features_csv_path = '/home/girish/Downloads/Zenodo/Video_features/zzXHxHz_WTr.csv'\n",
    "features_df.to_csv(features_csv_path, index=False)\n",
    "\n",
    "print(f\"Features saved to {features_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e01f1-b006-49d7-9875-70ad24d773fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# for whole folder video emaddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92a577-578e-4e32-b737-2acfa8e675fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import av\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "import os\n",
    "import glob\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def extract_video_features(file_path, device):\n",
    "    container = av.open(file_path)\n",
    "    indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "    \n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return file_path, features\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "video_folder = \"/home/girish/Downloads/Zenodo/VideoInterview\" \n",
    "video_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "\n",
    "all_features = []\n",
    "file_names = []\n",
    "for file_path in video_files:\n",
    "    file_name, features = extract_video_features(file_path, device)\n",
    "    file_names.append(os.path.basename(file_name)) \n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "features_df['file_name'] = file_names \n",
    "features_csv_path = '/home/girish/Downloads/Zenodo/Video_features/video_features.csv' \n",
    "features_df.to_csv(features_csv_path, index=False)\n",
    "\n",
    "print(f\"Features saved to {features_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a57be-0433-4ebf-9c36-c99f124910f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import av\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "import os\n",
    "import glob\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "video_folder = \"/home/girish/Downloads/Zenodo/VideoInterview\" \n",
    "features_csv_path = \"/home/girish/Downloads/Zenodo/Video_features/video_features.csv\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c9f750-76ac-4d98-85eb-a0e9e5812b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_all_frames(file_path):\n",
    "    \"\"\"\n",
    "    Read all video frames and return as a list of numpy arrays.\n",
    "    \"\"\"\n",
    "    container = av.open(file_path)\n",
    "    frames = [frame.to_image() for frame in container.decode(video=0)]\n",
    "    return frames\n",
    "\n",
    "def uniform_frame_sampling(frames, num_samples=16):\n",
    "    \"\"\"\n",
    "    Uniformly sample frames from the video.\n",
    "    \"\"\"\n",
    "    sampled_frames = [frames[i] for i in np.linspace(0, len(frames) - 1, num_samples, dtype=int)]\n",
    "    return sampled_frames\n",
    "\n",
    "def extract_and_pool_video_features(file_path, device, num_samples=16, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Extract and pool video features from uniformly sampled frames.\n",
    "    \"\"\"\n",
    "    frames = read_video_all_frames(file_path)\n",
    "    sampled_frames = uniform_frame_sampling(frames, num_samples)\n",
    "    \n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "    model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "    \n",
    "    pooled_features = []\n",
    "    for frame in sampled_frames:\n",
    "        inputs = image_processor(images=frame, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        pooled_features.append(features)\n",
    "    \n",
    "    if pooling == 'mean':\n",
    "        final_features = np.mean(pooled_features, axis=0)\n",
    "    elif pooling == 'max':\n",
    "        final_features = np.max(pooled_features, axis=0)\n",
    "    \n",
    "    return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4416f-862b-4e3a-b776-2e618d7de3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deprecated pixel format used, make sure you did set range correctly\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "deprecated pixel format used, make sure you did set range correctly\n",
      " (repeated 765 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/girish/Downloads/Zenodo/VideoInterview/CFWyWfu_SpR.mp4 due to error: Calculated padded input size per channel: (1 x 224 x 224). Kernel size: (2 x 16 x 16). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "def process_videos(video_files, device):\n",
    "    all_features = []\n",
    "    file_names = []\n",
    "    \n",
    "    for file_path in video_files:\n",
    "        try:\n",
    "            features = extract_and_pool_video_features(file_path, device)\n",
    "            file_names.append(os.path.basename(file_path))  \n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {file_path} due to error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_features, columns=[f'Feature_{i}' for i in range(all_features[0].shape[0])]), file_names\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "    features_df, file_names = process_videos(video_files, device)\n",
    "    features_df['file_name'] = file_names \n",
    "    features_df.to_csv(features_csv_path, index=False)\n",
    "\n",
    "    print(f\"Features saved to {features_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aab7a0-c6e4-40f3-83ab-3d919a5080dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN ON VIDEO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedbb10a-a8cb-4717-86c1-cc0db1afb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1f729-dbeb-4fb4-86cc-73867c02c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/girish/Downloads/Zenodo/Video_features/video_features.csv')\n",
    "data.drop(columns=['file_name'], inplace=True)\n",
    "\n",
    "X = data.drop(columns=['Extraversion', 'Agreeableness', 'Conscientiousness', 'Neuroticism', 'Openness']).values\n",
    "y = data[['Extraversion', 'Agreeableness', 'Conscientiousness', 'Neuroticism', 'Openness']].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4a7d8-f99f-4734-9f24-5190ab624e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 04:21:48.424040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.447176: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.447312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.469557: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.469630: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.469654: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.480780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.481819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.481923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-16 04:21:48.482699: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-16 04:21:48.483289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2841 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5)  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9738fef4-09c7-4d5a-acb2-6263dcfbad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 04:21:59.849734: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-02-16 04:22:02.197302: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:504] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-12.2\n",
      "  /usr/local/cuda\n",
      "  /home/girish/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/girish/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-02-16 04:22:04.506375: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb4edb176b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-16 04:22:04.506495: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2024-02-16 04:22:04.587050: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-16 04:22:04.796126: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.797663: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.820468: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.821989: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.924861: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.926532: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.949957: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:04.951451: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:05.035726: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:05.038112: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:06.010018: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:542] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2024-02-16 04:22:06.011337: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:574 : INTERNAL: libdevice not found at ./libdevice.10.bc\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall_4 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 518, in process_one\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_30106/1005542190.py\", line 1, in <module>\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1340, in apply_grad_to_update_var\n\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node Adam/StatefulPartitionedCall_4}}]] [Op:__inference_train_function_969]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall_4 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 518, in process_one\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_30106/1005542190.py\", line 1, in <module>\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n\n  File \"/home/girish/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1340, in apply_grad_to_update_var\n\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node Adam/StatefulPartitionedCall_4}}]] [Op:__inference_train_function_969]"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_reshaped, y_train, validation_split=0.2, epochs=100, batch_size=8, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8587ef8-0a33-4238-a43f-34ec98f3f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d53a0-179b-4820-8b4b-7a0607384672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
