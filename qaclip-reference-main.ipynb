{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:49.962669Z",
     "iopub.status.busy": "2023-12-07T08:36:49.961834Z",
     "iopub.status.idle": "2023-12-07T08:36:53.810056Z",
     "shell.execute_reply": "2023-12-07T08:36:53.809044Z",
     "shell.execute_reply.started": "2023-12-07T08:36:49.962627Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:53.813062Z",
     "iopub.status.busy": "2023-12-07T08:36:53.811964Z",
     "iopub.status.idle": "2023-12-07T08:36:58.201936Z",
     "shell.execute_reply": "2023-12-07T08:36:58.200953Z",
     "shell.execute_reply.started": "2023-12-07T08:36:53.813026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "json_file = \"/kaggle/input/new-json-file/train_new.json\"\n",
    "data = []\n",
    "with open(json_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data = []\n",
    "with open(json_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "train_df = pd.DataFrame(data)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.204110Z",
     "iopub.status.busy": "2023-12-07T08:36:58.203704Z",
     "iopub.status.idle": "2023-12-07T08:36:58.211356Z",
     "shell.execute_reply": "2023-12-07T08:36:58.210252Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.204073Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.214557Z",
     "iopub.status.busy": "2023-12-07T08:36:58.213661Z",
     "iopub.status.idle": "2023-12-07T08:36:58.226231Z",
     "shell.execute_reply": "2023-12-07T08:36:58.225363Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.214521Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.227602Z",
     "iopub.status.busy": "2023-12-07T08:36:58.227329Z",
     "iopub.status.idle": "2023-12-07T08:36:58.248160Z",
     "shell.execute_reply": "2023-12-07T08:36:58.247311Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.227578Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['question', 'anser', 'Quest_hindi', 'ans_hindi']]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.249853Z",
     "iopub.status.busy": "2023-12-07T08:36:58.249548Z",
     "iopub.status.idle": "2023-12-07T08:36:58.531574Z",
     "shell.execute_reply": "2023-12-07T08:36:58.530710Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.249827Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['anser'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.532901Z",
     "iopub.status.busy": "2023-12-07T08:36:58.532628Z",
     "iopub.status.idle": "2023-12-07T08:36:58.605888Z",
     "shell.execute_reply": "2023-12-07T08:36:58.604922Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.532877Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#hindi_ans = np.load(\"/kaggle/input/text-quest-ans-embed/Hindi_Ans_embd_new.npy\").squeeze(axis = 1)\n",
    "#print(hindi_ans.shape)\n",
    "hindi_ques = np.load(\"/kaggle/input/text-quest-ans-embed/Hindi_Quest_embd_new.npy\").squeeze(axis = 1)\n",
    "print(hindi_ques.shape)\n",
    "\n",
    "#english_ans = np.load(\"/kaggle/input/text-quest-ans-embed/English_Ans_embd_new.npy\").squeeze(axis = 1)\n",
    "#print(english_ans.shape)\n",
    "english_ques = np.load(\"/kaggle/input/text-quest-ans-embed/English_Quest_embd_new.npy\").squeeze(axis = 1)\n",
    "print(english_ques.shape)\n",
    "\n",
    "audio_feats = np.load(\"/kaggle/input/audio-stacked/Audio_stacked_array.npy\")\n",
    "print(audio_feats.shape)\n",
    "video_feats = np.load(\"/kaggle/input/video-stacked/stacked_array.npy\")\n",
    "print(video_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.607460Z",
     "iopub.status.busy": "2023-12-07T08:36:58.607072Z",
     "iopub.status.idle": "2023-12-07T08:36:58.713180Z",
     "shell.execute_reply": "2023-12-07T08:36:58.712165Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.607426Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ques_feats = hindi_ques\n",
    "\n",
    "X_v, X_testv, y_v, y_testv = train_test_split(video_feats,train_df['label_encoded'], test_size=0.2, random_state=42)\n",
    "X_a, X_testa, y_a, y_testa = train_test_split(audio_feats,train_df['label_encoded'], test_size=0.2, random_state=42)\n",
    "X_t, X_testt, y_t, y_testt = train_test_split(ques_feats,train_df['label_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_trainv, X_valv, y_trainv, y_valv = train_test_split(X_v,y_v, test_size=0.2, random_state=42)\n",
    "X_traina, X_vala, y_traina, y_vala = train_test_split(X_a, y_a, test_size=0.2, random_state=42)\n",
    "X_traint, X_valt, y_traint, y_valt = train_test_split(X_t,y_t, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:36:58.715383Z",
     "iopub.status.busy": "2023-12-07T08:36:58.714893Z",
     "iopub.status.idle": "2023-12-07T08:37:10.856267Z",
     "shell.execute_reply": "2023-12-07T08:37:10.855199Z",
     "shell.execute_reply.started": "2023-12-07T08:36:58.715347Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:10.861021Z",
     "iopub.status.busy": "2023-12-07T08:37:10.860685Z",
     "iopub.status.idle": "2023-12-07T08:37:11.460718Z",
     "shell.execute_reply": "2023-12-07T08:37:11.459882Z",
     "shell.execute_reply.started": "2023-12-07T08:37:10.860990Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "encoder1 = keras_nlp.layers.TransformerEncoder(intermediate_dim=120, num_heads=4)\n",
    "encoder2 = keras_nlp.layers.TransformerEncoder(intermediate_dim=120, num_heads=4)\n",
    "encoder3 = keras_nlp.layers.TransformerEncoder(intermediate_dim=120, num_heads=4)\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from numpy import newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:11.462031Z",
     "iopub.status.busy": "2023-12-07T08:37:11.461775Z",
     "iopub.status.idle": "2023-12-07T08:37:14.273041Z",
     "shell.execute_reply": "2023-12-07T08:37:14.272037Z",
     "shell.execute_reply.started": "2023-12-07T08:37:11.462009Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_video_encoder():\n",
    "    input_video = tf.keras.Input(shape=[768, 1])\n",
    "    x1 = tf.keras.layers.Conv1D(32, 3, activation = 'relu', padding = 'same', name= \"cnn1\")(input_video)\n",
    "    x1 = tf.keras.layers.MaxPooling1D(name= \"maxpool1\")(x1)\n",
    "    x1 = encoder1(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(120, activation= None)(x1)\n",
    "    return tf.keras.Model(inputs= input_video, outputs = x1, name = \"video_encoder\")\n",
    "\n",
    "encoder_v = create_video_encoder()\n",
    "print(encoder_v.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:14.274688Z",
     "iopub.status.busy": "2023-12-07T08:37:14.274388Z",
     "iopub.status.idle": "2023-12-07T08:37:14.460054Z",
     "shell.execute_reply": "2023-12-07T08:37:14.459092Z",
     "shell.execute_reply.started": "2023-12-07T08:37:14.274662Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_audio_encoder(): \n",
    "    input_audio = tf.keras.Input(shape=[768, 1])\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3, activation = 'relu', padding = 'same')(input_audio)\n",
    "    x2 = tf.keras.layers.MaxPooling1D()(x2)\n",
    "    x2 = encoder2(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Dense(120, activation= None)(x2)\n",
    "    return tf.keras.Model(inputs= input_audio, outputs = x2, name = \"audio_encoder\")\n",
    "\n",
    "encoder_a = create_audio_encoder()\n",
    "print(encoder_a.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:14.461610Z",
     "iopub.status.busy": "2023-12-07T08:37:14.461165Z",
     "iopub.status.idle": "2023-12-07T08:37:14.671467Z",
     "shell.execute_reply": "2023-12-07T08:37:14.670301Z",
     "shell.execute_reply.started": "2023-12-07T08:37:14.461576Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_text_encoder():     \n",
    "    input_text = tf.keras.Input(shape=[1024, 1])\n",
    "    x3 = tf.keras.layers.Conv1D(32, 3, activation = 'relu', padding = 'same')(input_text)\n",
    "    x3 = tf.keras.layers.MaxPooling1D()(x3)\n",
    "    x3 = encoder3(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(120, activation= None)(x3)\n",
    "    return tf.keras.Model(inputs= input_text, outputs = x3, name = \"text_encoder\")\n",
    "\n",
    "encoder_t = create_text_encoder()\n",
    "print(encoder_t.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:14.673482Z",
     "iopub.status.busy": "2023-12-07T08:37:14.673104Z",
     "iopub.status.idle": "2023-12-07T08:37:14.692413Z",
     "shell.execute_reply": "2023-12-07T08:37:14.691424Z",
     "shell.execute_reply.started": "2023-12-07T08:37:14.673449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main reference: https://keras.io/examples/vision/nl_image_search/\n",
    "# Secondary reference: https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2\n",
    "\n",
    "class TripleEncoder(keras.Model):\n",
    "    def __init__(self, video_encoder, audio_encoder, text_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.video_encoder = video_encoder\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        video_embeddings = self.video_encoder(features[\"video\"], training=training)\n",
    "        audio_embeddings = self.audio_encoder(features[\"audio\"], training=training)\n",
    "        text_embeddings = self.text_encoder(features[\"text\"], training=training)\n",
    "        return video_embeddings, audio_embeddings, text_embeddings\n",
    "\n",
    "    def compute_loss(self, embeddings1, embeddings2):\n",
    "        logits = tf.matmul(embeddings1, embeddings2, transpose_b=True) / self.temperature\n",
    "        similarity_matrix = tf.matmul(embeddings2, embeddings2, transpose_b=True)\n",
    "        targets = keras.activations.softmax(similarity_matrix / (2 * self.temperature))\n",
    "        loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            video_embeddings, audio_embeddings, text_embeddings = self(features, training=True)\n",
    "            video_audio_loss = self.compute_loss(video_embeddings, audio_embeddings)\n",
    "            video_text_loss = self.compute_loss(video_embeddings, text_embeddings)\n",
    "            audio_text_loss = self.compute_loss(audio_embeddings, text_embeddings)\n",
    "            total_loss = video_audio_loss + video_text_loss + audio_text_loss\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        video_embeddings, audio_embeddings, text_embeddings = self(features, training=False)\n",
    "        video_audio_loss = self.compute_loss(video_embeddings, audio_embeddings)\n",
    "        video_text_loss = self.compute_loss(video_embeddings, text_embeddings)\n",
    "        audio_text_loss = self.compute_loss(audio_embeddings, text_embeddings)\n",
    "        total_loss = video_audio_loss + video_text_loss + audio_text_loss\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:37:14.694438Z",
     "iopub.status.busy": "2023-12-07T08:37:14.693994Z",
     "iopub.status.idle": "2023-12-07T08:38:40.736310Z",
     "shell.execute_reply": "2023-12-07T08:38:40.735358Z",
     "shell.execute_reply.started": "2023-12-07T08:37:14.694406Z"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "video_data = np.random.rand(num_samples, 768)\n",
    "audio_data = np.random.rand(num_samples, 768)\n",
    "text_data = np.random.rand(num_samples, 1024)\n",
    "\n",
    "dataset = {\n",
    "    \"video\": video_data,\n",
    "    \"audio\": audio_data,\n",
    "    \"text\": text_data,\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "dataset = {\n",
    "    \"video\": X_v,\n",
    "    \"audio\": X_a,\n",
    "    \"text\": X_t,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(32)\n",
    "\n",
    "\n",
    "video_encoder = create_video_encoder()\n",
    "audio_encoder = create_audio_encoder()\n",
    "text_encoder = create_text_encoder()\n",
    "\n",
    "\n",
    "triple_encoder = TripleEncoder(video_encoder, audio_encoder, text_encoder, temperature=1.0)\n",
    "\n",
    "triple_encoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "history = triple_encoder.fit(tf_dataset, epochs=num_epochs)\n",
    "\n",
    "\n",
    "evaluation_result = triple_encoder.evaluate(tf_dataset)\n",
    "print(\"Evaluation Loss:\", evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T08:38:40.737945Z",
     "iopub.status.busy": "2023-12-07T08:38:40.737578Z",
     "iopub.status.idle": "2023-12-07T08:38:41.211477Z",
     "shell.execute_reply": "2023-12-07T08:38:41.210502Z",
     "shell.execute_reply.started": "2023-12-07T08:38:40.737915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting Downstream features out of a trained encoder\n",
    "\n",
    "trained_video_encoder = create_video_encoder()\n",
    "trained_video_encoder.set_weights(triple_encoder.video_encoder.get_weights())\n",
    "new_video_data = np.random.rand(10, 768)\n",
    "new_video_features = trained_video_encoder.predict(new_video_data)\n",
    "\n",
    "\n",
    "print(\"Extracted Video Features:\")\n",
    "print(new_video_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 8s 8s/step - loss: 3.4868\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.6079\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7586\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2144\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0160\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0017\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0714e-04\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8889e-05\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6062e-06\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.4033e-07\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7514e-07\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.1352e-08\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1015e-08\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2786e-09\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0769e-09\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8780e-10\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5203e-10\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4338e-11\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9201e-11\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4111e-11\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2264e-12\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.9167e-12\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2312e-12\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3306e-12\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.2861e-13\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.3674e-13\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.6024e-13\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4982e-13\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7877e-13\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3157e-13\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9480e-14\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.6937e-14\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.0851e-14\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9092e-14\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0355e-14\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.3716e-14\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.8657e-14\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4702e-14\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.1549e-14\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9031e-14\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7003e-14\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5341e-14\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3954e-14\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2814e-14\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1852e-14\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1040e-14\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0352e-14\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.7593e-15\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.2509e-15\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.8088e-15\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 3.1490e-21\n",
      "Evaluation Loss: 3.149036202311029e-21\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "video_features_df = pd.read_csv('C:/Users/giris/Downloads/RESEARCH/Personality/personality_data/Video_Interview_features.csv')\n",
    "audio_features_df = pd.read_csv('C:/Users/giris/Downloads/RESEARCH/Personality/personality_data/Extracted_Audio_features.csv')\n",
    "\n",
    "video_features = video_features_df.iloc[:, 1:-5].values  \n",
    "audio_features = audio_features_df.iloc[:, 1:-5].values \n",
    "\n",
    "video_features = (video_features - np.mean(video_features, axis=0)) / np.std(video_features, axis=0)\n",
    "audio_features = (audio_features - np.mean(audio_features, axis=0)) / np.std(audio_features, axis=0)\n",
    "\n",
    "\n",
    "split_index = int(0.8 * len(video_features)) \n",
    "train_video_features = video_features[:split_index]\n",
    "test_video_features = video_features[split_index:]\n",
    "\n",
    "train_audio_features = audio_features[:split_index]\n",
    "test_audio_features = audio_features[split_index:]\n",
    "\n",
    "# Creating TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"video\": train_video_features, \"audio\": train_audio_features}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\"video\": test_video_features, \"audio\": test_audio_features}))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "def create_video_encoder():\n",
    "    input_video = tf.keras.Input(shape=[video_features.shape[1]])  # Assuming shape of video features\n",
    "    x1 = tf.keras.layers.Reshape((video_features.shape[1], 1))(input_video)  # Adding a channel dimension\n",
    "    x1 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same', name=\"cnn1\")(x1)\n",
    "    x1 = tf.keras.layers.MaxPooling1D(name=\"maxpool1\")(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(120, activation=None)(x1)\n",
    "    return tf.keras.Model(inputs=input_video, outputs=x1, name=\"video_encoder\")\n",
    "\n",
    "def create_audio_encoder():\n",
    "    input_audio = tf.keras.Input(shape=[audio_features.shape[1]])  # Assuming shape of audio features\n",
    "    x2 = tf.keras.layers.Reshape((audio_features.shape[1], 1))(input_audio)  # Adding a channel dimension\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D()(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Dense(120, activation=None)(x2)\n",
    "    return tf.keras.Model(inputs=input_audio, outputs=x2, name=\"audio_encoder\")\n",
    "\n",
    "class TripleEncoder(tf.keras.Model):\n",
    "    def __init__(self, video_encoder, audio_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.video_encoder = video_encoder\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        video_embeddings = self.video_encoder(features[\"video\"], training=training)\n",
    "        audio_embeddings = self.audio_encoder(features[\"audio\"], training=training)\n",
    "        return video_embeddings, audio_embeddings\n",
    "\n",
    "    def compute_loss(self, embeddings1, embeddings2):\n",
    "        logits = tf.matmul(embeddings1, embeddings2, transpose_b=True) / self.temperature\n",
    "        similarity_matrix = tf.matmul(embeddings2, embeddings2, transpose_b=True)\n",
    "        targets = tf.keras.activations.softmax(similarity_matrix / (2 * self.temperature))\n",
    "        loss = tf.keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            video_embeddings, audio_embeddings = self(features, training=True)\n",
    "            video_audio_loss = self.compute_loss(video_embeddings, audio_embeddings)\n",
    "            total_loss = video_audio_loss\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        video_embeddings, audio_embeddings = self(features, training=False)\n",
    "        video_audio_loss = self.compute_loss(video_embeddings, audio_embeddings)\n",
    "        total_loss = video_audio_loss\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "video_encoder = create_video_encoder()\n",
    "audio_encoder = create_audio_encoder()\n",
    "\n",
    "triple_encoder = TripleEncoder(video_encoder, audio_encoder, temperature=1.0)\n",
    "\n",
    "triple_encoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "num_epochs = 50\n",
    "history = triple_encoder.fit(train_dataset, epochs=num_epochs)\n",
    "\n",
    "evaluation_result = triple_encoder.evaluate(test_dataset)\n",
    "print(\"Evaluation Loss:\", evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4098097,
     "sourceId": 7108018,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4098100,
     "sourceId": 7108021,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4098104,
     "sourceId": 7108027,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4098198,
     "sourceId": 7108167,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
